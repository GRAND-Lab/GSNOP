sampling:
  - layer: 2
    neighbor: 
      - 10
      - 10
    strategy: 'uniform'
    prop_time: True
    history: 3
    duration: 10000
    num_thread: 8
memory: 
  - type: 'none'
    dim_out: 0
gnn:
  - arch: 'transformer_attention'
    layer: 2
    att_head: 2
    dim_time: 0
    dim_out: 100
    combine: 'rnn'
np:
  - r_dim: 256
    z_dim: 256
    h_dim: 256
    y_dim: 128
    t_dim: 256
    out_dim: 256
    l: 10
    r_tol: 1e-5
    a_tol: 1e-7
    mem_size: 10
    method: "dopri5"
    context_split: 0.1
    old_as_context: true
train:
  - epoch: 999
    batch_size: 600
    test_batch_size: 100
    history_encoding_batch_size: 600
    lr: 0.00001
    dropout: 0.1
    train_neg_size: 1
    att_dropout: 0.1
    all_on_gpu: True
    patience: 5
    train_ratio: 0
    test_split_num: 5